---
layout: post_mine
title:  machine learning
---

![RoadMap](/images/mlcurve.jpg)

[NLP](https://en.wikipedia.org/wiki/Natural_language_processing)

---------------------------------------------------------------

# :star: 置顶

[机器学习的本质是什么？](http://www.zhihu.com/question/19830921)

[机器学习专家与统计学家观点上有哪些不同？](https://www.zhihu.com/question/29687860)

[机器学习该怎么入门？](https://www.zhihu.com/question/20691338)

[如何快速成为数据分析师](https://www.zhihu.com/question/29265587)

[机器学习有很多关于核函数的说法，核函数的定义和作用是什么？](https://www.zhihu.com/question/24627666)

[如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240)

[「社会网络分析」是怎样的学科?](https://www.zhihu.com/question/28939731)

[如何准备机器学习工程师的面试?](https://www.zhihu.com/question/23259302)

[自然语言处理怎么最快入门？](https://www.zhihu.com/question/19895141)

# PROJECTS

[OpenNLP](http://opennlp.apache.org/)

The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text.

It supports the most common NLP tasks, such as `tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution`. These tasks are usually required to build more advanced text processing services. OpenNLP also includes `maximum entropy` and `perceptron` based machine learning.

[CoreNLP](http://nlp.stanford.edu)

[Stanford Relation Extractor](http://nlp.stanford.edu/software/relationExtractor.html)

Stanford relation extractor is a Java implementation to find relations between two entities. The current relation extraction model is trained on the relation types (except the `kill` relation) and data from the paper `Roth and Yih, Global inference for entity and relation identification via a linear programming formulation, 2007`, except instead of using the gold NER tags, we used the NER tags predicted by [Stanford NER classifier](http://nlp.stanford.edu/software/CRF-NER.shtml) to improve generalization.

[语言技术平台LTP](http://www.ltp-cloud.com/document/)

[TweetNLP](http://www.cs.cmu.edu/~ark/TweetNLP/)

* [https://github.com/brendano/ark-tweet-nlp](https://github.com/brendano/ark-tweet-nlp)

[清华大学自然语言处理与社会人文计算实验室](http://nlp.csai.tsinghua.edu.cn/site2/)

* [http://www.cis.upenn.edu/~chinese/](http://www.cis.upenn.edu/~chinese/)

[UW NLPTools](https://github.com/knowitall/nlptools)

[Deep Learning of Natural Language Processing](http://cs224d.stanford.edu/index.html)

[Deep Learning of Natural Language Processing - Reports](http://cs224d.stanford.edu/reports.html)

# KEYWORD

* [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)
* [F1 score](https://en.wikipedia.org/wiki/F1_score)
* [线性分类器](https://en.wikipedia.org/wiki/Linear_classifier)

# 课程

* [Statistical analysis of neural data](http://www.stat.columbia.edu/~liam/teaching/neurostat-fall15/)

# TASK

* [Chunk](http://www.chokkan.org/software/crfsuite/tutorial.html)

> Text chunking divides a text into syntactically correlated parts of words. For example, the sentence “He reckons the current account deficit will narrow to only # 1.8 billion in September.” can be divided as follows:
> [NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP to ] [NP only # 1.8 billion ] [PP in ] [NP September ]

* [IOB2 notation](https://en.wikipedia.org/wiki/Inside_Outside_Beginning)

# :star: RESOURCE

* [http://www-nlp.stanford.edu/links/statnlp.html](http://www-nlp.stanford.edu/links/statnlp.html)
* [scipy](http://docs.scipy.org/doc/#)
* [Penn Treebank](http://www.cis.upenn.edu/~treebank/)
* [Senseval](http://www.senseval.org/)
* [HMM & EM Paper](http://www.stat.columbia.edu/~liam/teaching/neurostat-spr11/papers/)

# BOOKS

* [Elements of Statistical Learning](#)
* [周志华 机器学习](https://www.zhihu.com/question/39945249)
* [An Introduction to Statistical Learning](#) [Notes and Solution Manual](https://github.com/asadoughi/stat-learning)
* [Machine learning textbook](https://www.cs.ubc.ca/~murphyk/MLbook/)

> 非常非常详细，1000多页，⊙﹏⊙b汗，MCMC，Deep Learning等内容都有涉及，可以说囊括了机器学习里面大部分的模型和方法了，而且非常新，是12年初的，ESL是09年出的。MLAPP还有个很大的优点就是有详细的Matlab代码，可以边看边自己实践一遍。[GitHub - probml/pmtk3: Probabilistic Modeling Toolkit for Matlab/Octave](https://github.com/probml/pmtk3) 虽然三本书里面MLAPP是最全的，但是系统性感觉比不上ESL和PRML。

* [Solution Manual](http://waxworksmath.com/ce_solutionmanuals.asp)

> 很不错的学习笔记,可以看看.

* [The Handbook of Computational Linguistics and Natural Language Processing](https://books.google.com.hk/books?id=zBmom42eWPcC&pg=RA3-PT688&lpg=RA3-PT688&dq=Ratnaparkhi+thesis&source=bl&ots=N8F7p1qF62&sig=QS5ukHtyWeQYC_HOKsV1LfKZzmg&hl=zh-TW&sa=X&ved=0ahUKEwi05Pav-JvLAhWKl5QKHRXvDxcQ6AEIPTAE#v=onepage&q=Ratnaparkhi%20thesis&f=false)

# DISCUSSION

* [REDDIT NLP](https://www.reddit.com/r/languagetechnology)

* [REDDIT NLP](https://www.reddit.com/r/NLP/)

# 学习笔记

* [Markov Process](https://en.wikipedia.org/wiki/Markov_chain)
* [隐马尔科夫模型HMM自学](http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html) [2](http://blog.csdn.net/daringpig/article/details/8072794) [3](http://www.52nlp.cn/hmm-learn-best-practices-one-introduction)

# CoreNLP

* [分箱法](http://baike.baidu.com/view/3680824.htm)
* [http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binned_statistic.html](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binned_statistic.html)
* [google books](https://books.google.com/books?id=scua-qGbA2wC&pg=PA56&lpg=PA56&dq=%E5%88%86%E7%AE%B1%E6%95%B0%E6%8D%AE&source=bl&ots=qau3azwAUn&sig=ylo46akY_MZ8J3Hg7XVa4uAP7dY&hl=en&sa=X&ved=0ahUKEwis_LTVhrPLAhWFHB4KHZ8UBJ4Q6AEISDAF#v=onepage&q=%E5%88%86%E7%AE%B1%E6%95%B0%E6%8D%AE&f=false)

> 通过分箱离散化_数据挖掘：概念与技术（原书第3版）



